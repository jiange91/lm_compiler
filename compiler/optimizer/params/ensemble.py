from typing import Literal, Optional, Tuple
import uuid
import dataclasses
import heapq
import os
import sys
import json
import logging
from pathlib import Path
import copy


logger = logging.getLogger(__name__)

from compiler.IR.base import Module, StatePool, ModuleStatus
from compiler.IR.program import Workflow, Input, Output
from compiler.IR.modules import CodeBox
from compiler.IR.llm import LLMPredictor, Demonstration
from compiler.optimizer.params.common import EvolveType, ParamBase, ParamLevel, OptionBase, DynamicParamBase, IdentityOption, AddNewModuleImportInterface
from compiler.optimizer.decompose import LMTaskDecompose, StructuredAgentSystem
from compiler.langchain_bridge.interface import LangChainSemantic, LangChainLM, get_inspect_runnable, var_2_str
from compiler.optimizer.params.utils import dump_params, load_params
from compiler.optimizer.plugin import OptimizerSchema
from abc import ABC, ABCMeta

class ModuleEnsemble(ParamBase):
    level = ParamLevel.GRAPH
    
    @classmethod
    def from_dict(cls, data: dict):
        name, module_name, default_option, options = data['name'], data['module_name'], data['default_option'], data['options']
        options = [EnsembleOptionMeta.registry[dat['type']].from_dict(dat) for name, dat in options.items()]
        return cls(
            name=name,
            options=options,
            default_option=default_option,
            module_name=module_name,
        )
 
class EnsembleOptionMeta(ABCMeta):
    registry: dict[str, type] = {'IdentityOption': IdentityOption}
    
    def __new__(cls, name, bases, attrs):
        new_cls = super().__new__(cls, name, bases, attrs)
        cls.registry[name] = new_cls
        return new_cls   
        
class EnsembleOptionBase(OptionBase, metaclass=EnsembleOptionMeta):
    def __init__(
        self, 
        name: str, 
        num_path: int,
    ) -> None:
        super().__init__(name)
        self.num_path = num_path
    
    def sample_then_aggregate(self, module: Module) -> Module:
        raise NotImplementedError
    
    def apply(self, module: Module) -> Module:
        ensemble = self.sample_then_aggregate(module)
        return ensemble
    

class SamplerPostProcess(CodeBox):
    def __init__(
        self,
        name: str,
        origin_expert: LangChainLM,
        experts: list[LangChainLM],
    ):
        super().__init__(name=name, kernel=None)
        # Added this incase we will apply prompt rewriting for experts
        self.origin_expert = origin_expert
        self.experts = experts
    
    @property
    def prefix(self):
        return f'{self.name}_question_context'
    
    def invoke(self, statep: StatePool):
        # get all context of the problem
        agent_task = self.origin_expert.semantic.get_agent_role()
        
        inputs_dict = {}
        inputs = self.experts[0].step_info[-1]['inputs']
        for key, value in inputs.items():
            inputs_dict[key] = var_2_str(value)
            
        paths = []
        proposal_template = """
** Worker Proposal {i} **

Rationale: {rationale}

Answer: {response}
        """
        for i, expert in enumerate(self.experts):
            step = expert.step_info[-1]
            rationale = step.get('rationale', None)
            output = step['output']
            proposal = proposal_template.format(i=i, rationale=rationale, response=output)
            paths.append(proposal)
        paths_str = '\n---\n'.join(paths)
        question_context = {
            f"worker_task": agent_task,
            f"inputs": inputs_dict,
            f"proposals": paths_str,
        }
        statep.publish(question_context, self.version_id, self.is_static)
        self.version_id += 1
        self.status = ModuleStatus.SUCCESS

class UniversalSelfConsistency(EnsembleOptionBase):
    aggregator_system_prompt="""
You are tasked with aggregating multiple proposals generated by a worker agent in response to a specific task. Your job is to analyze the proposals, identify the points of agreement, and craft a final answer that reflects the majority consensus and maintains coherence. Where proposals conflict, your goal is to resolve discrepancies by selecting the most consistent or widely agreed-upon points. Ensure that the final answer is comprehensive, accurate, and respects the worker’s role and expertise.

You will be provided with:
- The role of that worker agent, since it is a LLM-agent so it will be its system prompt.
- Input to the worker agent, upon which the proposals were generated.
- A set of responses provided by the worker agent, each with potentially agreed or differing content. Each worker responses includes its reasoning and a final answer. 

Please read through all the responses carefully and provide a clear, consistent answer that respects the worker’s expertise and integrates the most consistent and widely agreed-upon content from the multiple proposals. 
"""

    def __init__(
        self,
        num_path: int,
        temperature: float = 0.7,
    ):
        super().__init__('universal_self_consistency', num_path)
        self.temperature = temperature
    
    def sample_then_aggregate(self, lm: LangChainLM) -> Module:
        sub_graph = Workflow(f'{lm.name}_ensemble_{self.name}')
        input_name, output_name = f'{lm.name}_sub_graph_input', f'{lm.name}_sub_graph_output'
        sub_graph.add_module(Input(input_name))
        sub_graph.add_module(Output(output_name))
        
        # Sampler
        lm_cpys = [copy.deepcopy(lm) for _ in range(self.num_path)]
        for i, lm_cpy in enumerate(lm_cpys):
            lm_cpy.name = f'{lm.name}_sampler_{i}'
            lm_cpy.lm_config.kwargs['temperature'] = self.temperature
            lm_cpy.reset()
            sub_graph.add_module(lm_cpy)
            sub_graph.add_edge(input_name, lm_cpy.name)
        
        sampler_post_process = SamplerPostProcess(
            name=f'{lm.name}_sampler_post_process',
            origin_expert=lm,
            experts=lm_cpys,
        )
        sub_graph.add_module(sampler_post_process)
        sub_graph.add_edge([lm_cpy.name for lm_cpy in lm_cpys], sampler_post_process.name)
        
        # Aggregator
        if lm.semantic.output_format:
            output_format = lm.semantic.output_format
        else:
            output_format = lm.semantic.get_agent_outputs()[0]
        output_format_instruction = "Please only give the final answer"
        if lm.semantic.output_format_instructions:
            output_format_instruction += f"\nAnswer format instructions given to the worker: {lm.semantic.output_format_instructions}\n Please strictly follow this as if you are generating the answer on worker's behalf."
        agg_semantic = LangChainSemantic(
            system_prompt=UniversalSelfConsistency.aggregator_system_prompt,
            inputs=[
                "worker_task",
                "inputs",
                "proposals",
            ],
            output_format=output_format,
            need_output_type_hint=lm.semantic.need_output_type_hint,
            output_format_instructions=output_format_instruction,
        )
        agg_lm = LangChainLM(f"{lm.name}_aggregator", agg_semantic)
        agg_lm.lm_config = copy.deepcopy(lm.lm_config)
        sub_graph.add_module(agg_lm)
        sub_graph.add_edge(sampler_post_process.name, agg_lm.name)
        
        sub_graph.compile()
        return sub_graph
    
    @classmethod
    def from_dict(cls, meta):
        num_path = meta['num_path']
        temperature = meta['temperature']
        return cls(num_path, temperature)
    
    def to_dict(self):
        base = super().to_dict()
        base['num_path'] = self.num_path
        base['temperature'] = self.temperature
        return base
        