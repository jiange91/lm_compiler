{
    "ae3d03380d9b46fe88d1d50e305c3681": {
        "id": "ae3d03380d9b46fe88d1d50e305c3681",
        "bo_trial_id": 0,
        "params": {
            "generate_answer_ensemble": "Identity",
            "refine_query_ensemble": "Identity",
            "generate_query_ensemble": "Identity"
        },
        "score": 0.7529523809523809,
        "price": 0.00019267799999999997,
        "eval_cost": 0.6193638,
        "next_level_log_dir": "/mnt/ssd4/lm_compiler/examples/HotPotQA/full_opt_refactored_mini/inner_loop/8eec2dac2ed44639822ce51587e1611c",
        "num_next_level_trials": 16
    },
    "1668f3a033b84d9298810bedeb1d5e7e": {
        "id": "1668f3a033b84d9298810bedeb1d5e7e",
        "bo_trial_id": 1,
        "params": {
            "generate_answer_ensemble": "Identity",
            "refine_query_ensemble": "Identity",
            "generate_query_ensemble": "universal_self_consistency"
        },
        "score": 0.640952380952381,
        "price": 0.000312612,
        "eval_cost": 0.5354604000000001,
        "next_level_log_dir": "/mnt/ssd4/lm_compiler/examples/HotPotQA/full_opt_refactored_mini/inner_loop/8b3c064af738476fb60883625c697e0c",
        "num_next_level_trials": 8
    },
    "f4429dbdcd784148927f14caeaa8a5e1": {
        "id": "f4429dbdcd784148927f14caeaa8a5e1",
        "bo_trial_id": 2,
        "params": {
            "generate_answer_ensemble": "universal_self_consistency",
            "refine_query_ensemble": "universal_self_consistency",
            "generate_query_ensemble": "universal_self_consistency"
        },
        "score": 0.7260460264373307,
        "price": 0.002728806,
        "eval_cost": 3.840186,
        "next_level_log_dir": "/mnt/ssd4/lm_compiler/examples/HotPotQA/full_opt_refactored_mini/inner_loop/effdb781741b45d2ab7529051b436fde",
        "num_next_level_trials": 16
    },
    "e0fee9deb11c4e49ace0fc9c2439d8eb": {
        "id": "e0fee9deb11c4e49ace0fc9c2439d8eb",
        "bo_trial_id": 3,
        "params": {
            "generate_answer_ensemble": "Identity",
            "refine_query_ensemble": "universal_self_consistency",
            "generate_query_ensemble": "universal_self_consistency"
        },
        "score": 0.6756190476190476,
        "price": 0.001887288,
        "eval_cost": 1.0334781,
        "next_level_log_dir": "/mnt/ssd4/lm_compiler/examples/HotPotQA/full_opt_refactored_mini/inner_loop/6e792823df2e4213add291e556d7cc5c",
        "num_next_level_trials": 8
    }
}