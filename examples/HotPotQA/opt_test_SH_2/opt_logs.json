{
    "da3218355de6494ab490a32dd469d6ad": {
        "id": "da3218355de6494ab490a32dd469d6ad",
        "bo_trial_id": 0,
        "params": {
            "generate_answer_ensemble": "Identity",
            "refine_query_ensemble": "Identity",
            "generate_query_ensemble": "Identity"
        },
        "score": 0.3850494333252954,
        "price": 0.00045648,
        "eval_cost": 0.00326412,
        "next_level_log_dir": "/mnt/ssd4/lm_compiler/examples/HotPotQA/opt_test_SH_2/inner_loop/34300bb6001c494d937927f7817387ce",
        "num_next_level_trials": 4
    },
    "016ad8201d2f403ebdb4c9f4fdc52ac3": {
        "id": "016ad8201d2f403ebdb4c9f4fdc52ac3",
        "bo_trial_id": 1,
        "params": {
            "generate_answer_ensemble": "universal_self_consistency",
            "refine_query_ensemble": "universal_self_consistency",
            "generate_query_ensemble": "universal_self_consistency"
        },
        "score": 0.4594965675057209,
        "price": 0.0018040500000000002,
        "eval_cost": 0.02022288,
        "next_level_log_dir": "/mnt/ssd4/lm_compiler/examples/HotPotQA/opt_test_SH_2/inner_loop/a9cd0723c172454a850cc0911dfd338c",
        "num_next_level_trials": 8
    },
    "4b25d03282b24b9ea86a4bb5340440cf": {
        "id": "4b25d03282b24b9ea86a4bb5340440cf",
        "bo_trial_id": 2,
        "params": {
            "generate_answer_ensemble": "universal_self_consistency",
            "refine_query_ensemble": "Identity",
            "generate_query_ensemble": "Identity"
        },
        "score": 0.6466422466422467,
        "price": 0.00076329,
        "eval_cost": 0.01386018,
        "next_level_log_dir": "/mnt/ssd4/lm_compiler/examples/HotPotQA/opt_test_SH_2/inner_loop/445109df126447ae9e9a86801f4a5fc2",
        "num_next_level_trials": 8
    },
    "92ffd2507e314eb980487f9da30a45f2": {
        "id": "92ffd2507e314eb980487f9da30a45f2",
        "bo_trial_id": 3,
        "params": {
            "generate_answer_ensemble": "Identity",
            "refine_query_ensemble": "universal_self_consistency",
            "generate_query_ensemble": "universal_self_consistency"
        },
        "score": 0.34261345062260395,
        "price": 0.00106881,
        "eval_cost": 0.00471534,
        "next_level_log_dir": "/mnt/ssd4/lm_compiler/examples/HotPotQA/opt_test_SH_2/inner_loop/42b1bef799f4474088d5173d9dbd145f",
        "num_next_level_trials": 4
    }
}