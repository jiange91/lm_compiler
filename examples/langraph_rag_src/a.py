from langchain_core.pydantic_v1 import BaseModel, Field
from typing import Union, Type
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
import json
from compiler.utils import load_api_key
load_api_key('secrets.toml')
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'examples'))

from kalmv import kalmv_semantic
from langchain_openai import ChatOpenAI
from compiler.langchain_bridge.interface import LangChainSemantic
from devtools import pprint

class AgentMeta(BaseModel):
    """Information about each agent"""
    inputs: list[str] = Field(
        description="list of inputs for the agent"
    )
    outputs: list[str] = Field(
        description="list of outputs for the agent"
    )
    prompt: str = Field(
        description="refined prompt for the agent"
    )
    next_action: Union[str, list[str]] = Field(
        description="next agents to invoke or python code for dynamic decision"
    )
    
class NewAgents(BaseModel):
    """New agent system"""
    agents: dict[str, AgentMeta] = Field(
        description="dictionary of agent name to information about that agent"
    )
    
dependency_system = """
You are an expert at designing LLM multi-agent workflow. Your task is to rewrite a single-LLM-agent system following some given guidance. 

You will be provided with information about the existing agent system, including its input/output variable name, and the high-level prompt. The guidance is a group of suggested agents you should use in the new system. Each includes a name and a prompt. They should collaboratively achieve the same goal as the original system. You need to decide how suggested new agents interact with each other. 

Specifically, for each new agent:
1. Give the input/output variable name of each new agent. This is used to retrieve/store the input/output value from/to external dictionary.
2. Decide the set of agents that will take action next. Use 'END' as the agent name to indicate no next agent to invoke. If next agents to be invoked are static, i.e. regardless of any factor, direclty provide their name in a list. Otherwise, write a python method for deciding this at the runtime. This is important as invoking wrong/unrelated agents can lead to incorrect results and waste compute budget.
3. Enrich the proposed prompt if necessary so that the agent can learn to take advantage of its input and also provide the expected output.

Please don't omit any suggested agent in your final answer. Especially for the next action, the answer should decide next action for each agent.

Be careful with the input/output variable names of new agents. 
 - Keep in mind that the new system will replace the existing one, so you should only use variables that present in the original agent system or ones generated by agents in the new system. 
 - Make sure all output names of the original system will always be generated by the new agent system no matter what control flow is taken.

As an example:

## Information of the existing agent system
### agent prompt
{{
You are an expert at software development. Given the user requirements, write software with good design and performance.
}}

### Original inputs 
{{
['requirements']
}}

### Original outputs
{{
['code']
}}

## Guidance for the new system
### Suggested new agents, with name and prompt
{{
"DesignAgent": "You are an expert at designing software. Given the user requirements, design the software with good architecture and performance.",
"CodeAgent": "You are an expert at writing code. Given the user requirement, write the code that meets the requirements.",
"ReviewAgent": "You are an expert at reviewing code. Review it for any bugs or issues."
}}

Peudo Output in this case, this is a good example to help you reason about the matter:

{example_json_output}
"""

example_json_output = """
{
  "agents": {
    "DesignAgent": {
      "inputs": ["requirements"],
      "outputs": ["design"],
      "prompt": "You are an expert at designing software. Given the user requirements, design the software with good architecture and performance.",
      "next_action": ["CodeAgent"]
    },
    "CodeAgent": {
      "inputs": ["requirements", "design"],
      "outputs": ["code"],
      "prompt": "You are an expert at writing code. Given the user requirements and design, write the code that meets the requirements and design.",
      "next_action": ["ReviewAgent"]
    },
    "ReviewAgent": {
      "inputs": ["requirements", "design", "code"],
      "outputs": ["decision"],
      "prompt": "You are an expert at reviewing code. Given the user requirements, design, and code, review it for any bugs or issues. If the code is acceptable, output 'accept'; otherwise, output 'reject'.",
      "next_action": "def next_agent(decision): return ['END'] if decision == 'accept' else ['CodeAgent']"
    }
  }
}
"""

user_prompt = """

Now, this is the real user question for you:

### Existing agent prompt
{{
{old_prompt}
}}

### Original inputs
{{
{inputs}
}}

### Original outputs
{{
{outputs}
}}

### Suggested new agents, with name: prompt
{new_agents}

Your answer:
"""

new_agents_dict =  {
"Question Evaluation Agent": "You are tasked with evaluating whether the provided answer addresses the user question. If it does not, you should grade the answer as 'ae' (Answer does not address the question). If the answer addresses the question, you should pass the evaluation to the next agent.",
"Knowledge Relevance Agent": "Your role is to evaluate the relevance of the provided knowledge to the user question. If the knowledge is irrelevant to the question, grade it as 're' (Relevant knowledge is absent). If the knowledge is relevant, pass the evaluation to the next agent.",
"Grounding Evaluation Agent": "You are responsible for assessing whether the provided answer is grounded in the relevant knowledge. If the answer is not grounded by the knowledge, grade it as 'ge' (Grounding is missing). If it is grounded, grade the answer as 'accept' (The answer is acceptable)."
}

interaction_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", dependency_system),
        ("human", user_prompt),
    ]
)

def decompose_kernel():
    llm = ChatOpenAI(model="gpt-4o", temperature=0.0)
    routine = interaction_prompt | llm | StrOutputParser()
    new_interaction = routine.invoke({
        "example_json_output": example_json_output,
        "old_prompt": kalmv_semantic.get_agent_role(), 
        "inputs": kalmv_semantic.get_agent_inputs(),
        "outputs": kalmv_semantic.get_agent_outputs(),
        "new_agents": new_agents_dict,
        }
    )
    print(new_interaction)
    interaction_prompt.extend(
        [
            ("ai", "{new_interaction}"),
            ("human", "Please reformat the your answer in the desired format."),
        ]
    )
    
    sllm = ChatOpenAI(model="gpt-4o", temperature=0.0).with_structured_output(NewAgents, method="json_mode")
    reformater = interaction_prompt | sllm
    soutput = reformater.invoke({
        "example_json_output": example_json_output,
        "old_prompt": kalmv_semantic.get_agent_role(), 
        "inputs": kalmv_semantic.get_agent_inputs(),
        "outputs": kalmv_semantic.get_agent_outputs(),
        "new_agents": new_agents_dict,
        "new_interaction": new_interaction,
        }
    )
    return soutput

# new_agents = decompose_kernel()

# pprint(new_agents)

# with open('a.json', 'w') as f:
#     json.dump(json.loads(new_agents.json()), f, indent=4)
# exit()

class Score(BaseModel):
    """Ratings of a research paper"""
    presentation: int = Field(
        description="Presentation of the research paper"
    )
    novelty: int = Field(
        description="Novelty of the research paper"
    )

class PaperReviews(BaseModel):
    """Reviews a list of research papers"""
    scores: dict[str, Score] = Field(
        description="dictionary of paper title to its score"
    )
 
example_semantic = LangChainSemantic(
    system_prompt="You are an expert at reviewing research papers. You are tasked with evaluating the presentation and novelty of each paper.",
    inputs=["papers"],
    output_format=PaperReviews,
)

print(example_semantic.get_formatted_info())
# exit()

class RelevanceScoreSchema(BaseModel):
    """Binary score for relevance check on retrieved documents."""
    binary_score: str = Field(
        description="Documents are relevant to the question, 'yes' or 'no'"
    )

class ToxicScoreSchema(BaseModel):
    """Binary score for toxicity check on retrieved documents."""
    binary_score: str = Field(
        description="Documents are toxic, 'yes' or 'no'"
    )
    
print(json.dumps(json.loads(RelevanceScoreSchema.schema_json()), indent=4))
print(json.dumps(json.loads(ToxicScoreSchema.schema_json()), indent=4))

class PresentationScoresSchema(BaseModel):
    """Scores for presentation of research papers"""
    presentation_scores: dict[str, int] = Field(
        description="dictionary of paper title to its score"
    )
    
class NoveltyScoresSchema(BaseModel):
    """Scores for novelty of research papers"""
    novelty_scores: dict[str, int] = Field(
        description="dictionary of paper title to its score"
    )
print(json.dumps(json.loads(PresentationScoresSchema.schema_json()), indent=4)) 
print(json.dumps(json.loads(NoveltyScoresSchema.schema_json()), indent=4))


def combine_outputs(presentation_scores: dict[str, int], novelty_scores: dict[str, int]):
    scores = {}
    for title in presentation_scores:
        scores[title] = {"presentation": presentation_scores[title], "novelty": novelty_scores[title]}
    return PaperReviews(scores=scores)

ps = PresentationScoresSchema(presentation_scores={"paper1": 5, "paper2": 4})
ns = NoveltyScoresSchema(novelty_scores={"paper1": 3, "paper2": 2})

pns = combine_outputs(ps.presentation_scores, ns.novelty_scores)
print(pns.json())