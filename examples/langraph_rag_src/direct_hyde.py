from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph, START
from compiler.langchain_bridge.interface import LangChainSemantic, LangChainLM
from schemas import *

class Unit(BaseModel):
    """return the topics/passages generated by HyDE"""
    topic: str = Field(
        description="a sub-topic generated"
    )
    passages: str = Field(
        description="passages generated for the sub-topic"
    )

class DirectHyde(BaseModel):
    """return the topics/passages generated by HyDE"""
    tps: list[Unit] = Field(
        description="a list of sub-topics and their corresponding passages"
    )
    
#------------------------ Old kernel ------------------------#
parser = PydanticOutputParser(pydantic_object=DirectHyde)

# HyDE document genration
template = """
You are an expert at writing a list of short passages given a user query. You should give sub-queries that cover comprehensive aspects of the original query and should not have too many overlaps. Each sub-query should be followed by a short passage that answers that topic.
Please answer with json format as follows:
{format_instructions}

User query: {question}
Answer:
"""
prompt_hyde = ChatPromptTemplate.from_template(template).partial(format_instructions=parser.get_format_instructions())

def direct_hyde_kernel(llm, question):
    sllm = llm.with_structured_output(DirectHyde)
    hyde = prompt_hyde | sllm 
    tps = hyde.invoke({"question": question}).tps
    sub_questions, passages = [], []
    for tp in tps:
        sub_questions.append(tp.topic)
        passages.append(tp.passages)
    return {'sub_questions': sub_questions, 'passages': passages}

#------------------------ New Semantic ------------------------#
system_prompt = """
You are an expert at writing a list of short passages given a user query. 
You should first give sub-queries that cover comprehensive aspects of the original query and should not have too many overlaps. 
For each sub-query, provide a short passage that answers that topic.
"""


direct_hyde_semantic = LangChainSemantic(
    system_prompt=system_prompt,
    inputs=["question"],
    output_format=HypertheticalPassages,
)

direct_hyde_module = LangChainLM('direct_hyde', direct_hyde_semantic)