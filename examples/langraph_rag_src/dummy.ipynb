{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compiler.utils import load_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_api_key('../secrets.toml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from self_eval import evaluate_rag_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/ssd4/lm_compiler/my_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/mnt/ssd4/lm_compiler/my_env/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_rag_answer(\"What are the types of agent memory?\", \n",
    "                    \"Agent memory in the context of artificial intelligence or autonomous agents can be categorized into several types, each serving a distinct function to enhance the agent's ability to store, retrieve, and utilize information effectively. These types include:\\n\\n1. **Sensory Memory**: This type of memory involves learning embedding representations for raw inputs. It acts as the initial stage where sensory data is processed and converted into a format that can be used by the agent.\\n\\n2. **Short-Term Memory**: Short-term memory is used for in-context learning and is limited by the finite context window length of the Transformer. It allows the agent to hold and manipulate information temporarily to make immediate decisions or actions.\\n\\n3. **Long-Term Memory**: Long-term memory is crucial for storing information that the agent may need to access over extended periods. It includes:\\n   - **Semantic Memory**: This involves storing embedding representations of information in an external vector store, which can be accessed via fast retrieval methods like Maximum Inner Product Search (MIPS). Semantic memory helps the agent to quickly find and use relevant information.\\n   - **Episodic Memory**: Episodic memory allows the agent to recall specific past experiences and events. This helps the agent to learn from previous interactions and apply that knowledge to similar future scenarios.\\n   - **Procedural Memory**: Procedural memory refers to the unconscious memory of skills and routines that are performed automatically. It involves the acquisition and retrieval of skills and procedures without conscious awareness, similar to implicit memory in humans.\\n\\nThese different types of memory interact within an agent's architecture to create a cohesive system. Sensory memory processes raw inputs into embeddings, which are then used in short-term memory for immediate tasks. Long-term memory stores these embeddings in an external vector store, allowing the agent to retrieve relevant information efficiently using methods like MIPS and ANN algorithms. This integrated memory system enables the agent to overcome the limitations of short-term memory and make informed decisions based on past experiences and learned knowledge.\", \n",
    "[\n",
    "                \"Agent memory in the context of artificial intelligence or autonomous agents refers to the mechanisms by which an agent stores and retrieves information to inform its behavior. It includes sensory memory for raw inputs, short-term memory for in-context learning, and long-term memory stored in an external vector database for fast retrieval. This memory system allows the agent to use past experiences and observations to make decisions and interact effectively.\",\n",
    "                \"The different types of short-term memory used by agents include sensory memory, which involves learning embedding representations for raw inputs, and short-term memory as in-context learning, which is limited by the finite context window length of the Transformer.\",\n",
    "                \"The different types of long-term memory used by agents are semantic memory, episodic memory, and procedural memory.\",\n",
    "                \"Agents use episodic memory to recall specific past experiences and events, which helps them make informed decisions and adapt to new situations. This type of memory allows them to learn from previous interactions and apply that knowledge to similar future scenarios.\",\n",
    "                \"Agents use semantic memory by storing embedding representations of information in an external vector store, which can be accessed via fast retrieval methods like Maximum Inner Product Search (MIPS). This allows them to overcome the limitations of finite attention spans. Common algorithms for fast MIPS include approximate nearest neighbors (ANN) to quickly find relevant information.\",\n",
    "                \"Procedural memory in the context of agent memory refers to the unconscious memory of skills and routines that are performed automatically. This type of memory is akin to implicit memory in humans, such as riding a bike or typing on a keyboard. It involves the acquisition and retrieval of skills and procedures without conscious awareness.\",\n",
    "                \"Yes, there are specialized types of memory used by agents, including sensory memory for learning embedding representations, short-term memory for in-context learning, and long-term memory as an external vector store accessible via fast retrieval methods like Maximum Inner Product Search (MIPS).\",\n",
    "                \"Different types of memory interact within an agent's architecture by leveraging sensory memory to create embeddings from raw inputs, which are then used in short-term memory for in-context learning within the finite context window of a Transformer. Long-term memory stores these embeddings in an external vector store, which can be accessed via fast retrieval methods like Maximum Inner Product Search (MIPS) using approximate nearest neighbors (ANN) algorithms. This setup allows the agent to overcome the limitations of short-term memory by efficiently retrieving relevant information from long-term memory.\"\n",
    "            ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
