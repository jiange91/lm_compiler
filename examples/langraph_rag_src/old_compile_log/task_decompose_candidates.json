{
    "direct_hyde": {
        "score": 4,
        "rationale": "Prompt 1 requires the agent to decompose a user query into multiple sub-queries that cover comprehensive aspects of the original query without too much overlap. This involves multi-step reasoning and planning to ensure the sub-queries are distinct and cover all necessary aspects. Additionally, generating short passages for each sub-query adds another layer of complexity, making at least one sub-task non-trivial."
    },
    "kalmv": {
        "score": 3,
        "rationale": "Prompt 3 requires evaluating the quality of an LLM-generated answer based on specific criteria. This involves vertical planning and decision-making to follow the grading order. However, each sub-task (checking if the answer addresses the question, if the knowledge is relevant, and if the answer is grounded) is relatively simple, making the overall complexity moderate."
    },
    "answer_compose": {
        "score": 2,
        "rationale": "Prompt 2 involves answering a user question based on provided Q&A pairs. While the task requires understanding the context and synthesizing an answer, the sub-tasks are relatively straightforward as the answers are already provided by experts. The main challenge is to refer to the correct Q&A pairs, which is a simple horizontal task."
    },
    "doc_filter_lm": {
        "score": 2,
        "rationale": "Prompt 5 involves assessing the relevance of a document to a user question. This is a simple binary decision-making task based on the presence of keywords or semantic meaning. While it involves multiple horizontal sub-tasks (checking for keywords or semantic meaning), each sub-task is simple to tackle."
    },
    "sub_answer_generator": {
        "score": 1,
        "rationale": "Prompt 4 is straightforward as it involves answering a question using provided context. The task is simple and clear, requiring the agent to synthesize a concise answer or state that it doesn't know the answer if the context is insufficient."
    }
}