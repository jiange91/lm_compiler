from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate


def format_qa_pairs(sub_questions, knowledge_answer):
    """Format Q and A pairs"""
    
    formatted_string = ""
    for i, (question, answer) in enumerate(zip(sub_questions, knowledge_answer), start=1):
        formatted_string += f"Question {i}: {question}\nAnswer {i}: {answer}\n\n"
    return formatted_string.strip()

# Prompt
system = """
You are an expert in answering user questions. You will be provided with a knowledge context and an user question.
The provided context is in a format of Q&A pairs. Questions in this set cover different aspect of the main question. Answers to these questions are written by experts, please refer to them when answer the user question.
"""

answer_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", "QA Context:\n{context}\n\nMain question:\n{question}\n\nAnswer:\n"),
    ]
)

def answer_compose_kernel(llm, question, sub_questions, sub_answers):
    context = format_qa_pairs(sub_questions, sub_answers)

    final_rag_chain = (
        answer_prompt
        | llm
        | StrOutputParser()
    )

    final_answer = final_rag_chain.invoke({"context": context, "question": question})
    return {'answer': final_answer}

#------------------------ New Semantic ------------------------#
from compiler.langchain_bridge.interface import LangChainSemantic, LangChainLM
from compiler.IR.modules import Map, CodeBox
from langchain_core.pydantic_v1 import BaseModel, Field

def preprocess_kernel(sub_questions, knowledge):
    return {'knowledge_context': format_qa_pairs(sub_questions, knowledge)}
    
    
knowledge_preprocess = CodeBox(
    name="knowledge_preprocess",
    kernel=preprocess_kernel,
)

class FinalAnswer(BaseModel):
    """return the answer generated by RAG"""
    answer: str = Field(
        description="answer for the main question"
    )
    
answer_compose_semantic = LangChainSemantic(
    system_prompt=system,
    inputs=['question', 'knowledge_context'],
    output_format=FinalAnswer,
)

answer_compose_lm = LangChainLM(
    name="answer_compose",
    semantic=answer_compose_semantic,
)
