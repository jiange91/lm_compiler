{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the Accuracy of Document Retrival\n",
    "\n",
    "In this example, we build a simple workflow for solving [MATH problems](https://github.com/hendrycks/math). \n",
    "\n",
    "**Example Problen and Solution:**\n",
    "\n",
    "P: The Smith family has 4 sons and 3 daughters. In how many ways can they be seated in a row of 7 chairs such that at least 2 boys are next to each other?\n",
    "\n",
    "S: This problem is a perfect candidate for complementary counting.  It will be fairly difficult to try to count this directly, since there are lots of possible cases (just two are BBBBGGG and BGGBBGB, where B is a boy and G is a girl).  But there is only one way to assign genders to the seating so that no two boys are next to each other, and that is BGBGBGB. If we seat the children as BGBGBGB, then there are $4!$ orderings for the 4 boys, and $3!$ orderings for the 3 girls, giving a total of $4! \\times 3! = 144$ seatings for the 7 children. These are the seatings that we don't want, so to count the seatings that we do want, we need to subtract these seatings from the total number of seatings without any restrictions.  Since there are 7 kids, there are $7!$ ways to seat them. So the answer is \n",
    "\n",
    "```math\n",
    "7! - (4! \\times 3!) = 5040-144 = \\boxed{4896}\n",
    "```\n",
    "\n",
    "\n",
    "The workflow involves 2 agents:\n",
    "- **Interpreter agent**: analyzes the problem and models the problem with equations.\n",
    "- **Solver agent**: focuses on solving the generated model to get the answer.\n",
    "\n",
    "![math](../imgs/math.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Setup\n",
    "\n",
    "First, let's set the environment for workflow execution. We use openai model in this example, please set your key in `.env` file as:\n",
    "\n",
    "OPENAI_API_KEY=\"your-openai-key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Check Math Workflow\n",
    "\n",
    "The implementation is based on `langchain` and is avaibale in `workflow.py`. \n",
    "\n",
    "Try it out with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': \"To solve the problem, we need to analyze the pattern of the student's actions as he opens the lockers.\\n\\n1. **Initial Setup**: There are 1024 lockers, all initially closed.\\n\\n2. **First Pass**: The student opens locker 1, then skips locker 2, opens locker 3, skips locker 4, and so on. This means he opens all odd-numbered lockers on the first pass:\\n   - Opened lockers: 1, 3, 5, ..., 1023 (total of 512 lockers).\\n\\n3. **Turning Around**: When he reaches locker 1024, he turns around and starts back. The first closed locker he encounters is locker 2 (since all odd-numbered lockers are open). He opens locker 2, then skips locker 4, opens locker 6, skips locker 8, and continues this pattern:\\n   - Opened lockers: 2, 6, 10, ..., 1022 (total of 256 lockers).\\n\\n4. **Subsequent Passes**: The student continues this process, alternating between opening the first closed locker he encounters and then skipping the next one. Each time he turns around, he opens lockers in a specific pattern:\\n   - On the third pass, he will open lockers 4, 12, 20, ..., and so on.\\n   - On the fourth pass, he will open lockers 8, 24, 40, ..., and so on.\\n\\n5. **General Pattern**: Each pass can be described as opening lockers that are multiples of \\\\(2^n\\\\) where \\\\(n\\\\) is the pass number (starting from 0). The number of lockers opened in each pass decreases as follows:\\n   - 1st pass: \\\\(2^0 = 1\\\\) (opened 512 lockers)\\n   - 2nd pass: \\\\(2^1 = 2\\\\) (opened 256 lockers)\\n   - 3rd pass: \\\\(2^2 = 4\\\\) (opened 128 lockers)\\n   - 4th pass: \\\\(2^3 = 8\\\\) (opened 64 lockers)\\n   - 5th pass: \\\\(2^4 = 16\\\\) (opened 32 lockers)\\n   - 6th pass: \\\\(2^5 = 32\\\\) (opened 16 lockers)\\n   - 7th pass: \\\\(2^6 = 64\\\\) (opened 8 lockers)\\n   - 8th pass: \\\\(2^7 = 128\\\\) (opened 4 lockers)\\n   - 9th pass: \\\\(2^8 = 256\\\\) (opened 2 lockers)\\n   - 10th pass: \\\\(2^9 = 512\\\\) (opened 1 locker)\\n\\n6. **Final Pass**: The last locker opened will be the last locker he encounters on the final pass. Since he opens lockers in the order of their numbers, the last locker opened will be locker 1024.\\n\\nThus, the number of the last locker he opens is:\\n\\n\\\\[\\n\\\\boxed{1024}\\n\\\\]\"}\n"
     ]
    }
   ],
   "source": [
    "%run workflow.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Optimize The Workflow\n",
    "\n",
    "The workflow entry point is already registered using annotation `cognify.register_workflow`.\n",
    "\n",
    "Here we configure the optimization pipeline:\n",
    "1. Define the evaluation method\n",
    "2. Define the data loader\n",
    "3. Config the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Use LLM-as-judge\n",
    "\n",
    "As you can see the standard solution includes both the result and the detailed steps required to achieve it. We utilize an LLM agent to evaluate the generated output for completeness and correctness.\n",
    "\n",
    "The agent assigns a score on a scale of 0 to 10, accounting for partially correct answers.\n",
    "\n",
    "We implement the scoring agent with `langchain` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cognify\n",
    "\n",
    "from pydantic import BaseModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Initialize the model\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Force agent to respond with a score\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "class Assessment(BaseModel):\n",
    "    score: int\n",
    "    \n",
    "parser = PydanticOutputParser(pydantic_object=Assessment)\n",
    "\n",
    "@cognify.register_evaluator\n",
    "def llm_judge(problem, answer, solution):\n",
    "    evaluator_prompt = \"\"\"\n",
    "You are a math problem evaluator. Your task is to grade the the answer to a math proble by assessing its correctness and completeness.\n",
    "\n",
    "You should not solve the problem by yourself, a standard solution will be provided. \n",
    "\n",
    "Please rate the answer with a score between 0 and 10.\n",
    "    \"\"\"\n",
    "    evaluator_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", evaluator_prompt),\n",
    "            (\"human\", \"problem:\\n{problem}\\n\\nstandard solution:\\n{solution}\\n\\nanswer:\\n{answer}\\n\\nYou response format:\\n{format_instructions}\\n\"),\n",
    "        ]\n",
    "    )\n",
    "    evaluator_agent = evaluator_template | model | parser\n",
    "    assess = evaluator_agent.invoke({\"problem\": problem, \"answer\": answer, \"solution\": solution, \"format_instructions\": parser.get_format_instructions()})\n",
    "    return assess.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load the Data\n",
    "\n",
    "We provide the subsampled math data in `data._json` file for you to start with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should be formatted to align with the function signature of both the workflow entry point and the evaluator.\n",
    "\n",
    "Signatures are:\n",
    "- workflow (problem) -> {'answer': ...}\n",
    "- evaluator (problem, answer, solution) -> int\n",
    "\n",
    "The workflow input expects `problem` field and will forward `answer` to the evaluator. \n",
    "\n",
    "Additionally, data loader needs to provide ground truth as `solution` to match the signature. \n",
    "\n",
    "With above rule, each data item should be formatted a tuple of (input, ground truth), each being a dictionary with required fields:\n",
    "\n",
    "```python\n",
    "(\n",
    "    {'problem': ...},\n",
    "    {'solution': ...}\n",
    ")\n",
    "```\n",
    "\n",
    "The complete data loader code is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "@cognify.register_data_loader\n",
    "def load_data():\n",
    "    with open(\"data._json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    random.seed(42)\n",
    "    random.shuffle(data) \n",
    "    # format to (input, output) pairs\n",
    "    new_data = []\n",
    "    for d in data:\n",
    "        input = {\n",
    "            'problem': d[\"problem\"],\n",
    "        }\n",
    "        ground_truth = {\n",
    "            'solution': d[\"solution\"],\n",
    "        }\n",
    "        new_data.append((input, ground_truth))\n",
    "    return new_data[:30], None, new_data[30:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Config the optimizer\n",
    "\n",
    "Let's use the default configuration to optimize this workflow. \n",
    "\n",
    "Additionally, the original workflow use `gpt-4o` for both agents, we also want to tune the model selection to save cost.\n",
    "\n",
    "The final search space:\n",
    "- 2 fewshot examples to add for each agent\n",
    "- whether to apply Chain-of-thought to each agent\n",
    "- select `gpt-4o` or `gpt-4o-mini` for each agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cognify.hub.search import default\n",
    "\n",
    "model_configs = [\n",
    "    # OpenAI models\n",
    "    cognify.LMConfig(model='gpt-4o-mini', kwargs={'temperature': 0, 'max_tokens': 300}),\n",
    "    cognify.LMConfig(model='gpt-4o', kwargs={'temperature': 0, 'max_tokens': 300}),\n",
    "]\n",
    "\n",
    "search_settings = default.create_search(\n",
    "    model_selection_cog=model_configs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Start the Optimization\n",
    "\n",
    "You can save the above configs in `config.py` file and use Cognify's CLI to fire the optimization with:\n",
    "\n",
    "```console\n",
    "$ cognify optimize workflow.py\n",
    "```\n",
    "\n",
    "Alternatively you can run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, dev = load_data()\n",
    "\n",
    "opt_cost, pareto_frontier, opt_logs = cognify.optimize(\n",
    "    script_path=\"workflow.py\",\n",
    "    control_param=search_settings,\n",
    "    train_set=train,\n",
    "    val_set=val,\n",
    "    eval_fn=llm_judge,\n",
    "    force=True, # This will overwrite the existing results\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
