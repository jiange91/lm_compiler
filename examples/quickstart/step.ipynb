{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cognify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================\n",
    "# Evaluator\n",
    "#================================================================\n",
    "\n",
    "# metric = cognify.metric.F1Str()\n",
    "lm_config = cognify.llm.LMConfig(\n",
    "    model='gpt-4o-mini',\n",
    ")\n",
    "metric = cognify.metric.AgentJudge(\n",
    "    criterion=\"The given output should have the same statement as the ground truth.\",\n",
    "    model_config=lm_config,\n",
    "    need_ground_truth=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================\n",
    "# Data Loader\n",
    "#================================================================\n",
    "\n",
    "import json\n",
    "\n",
    "def load_data_minor():\n",
    "    with open(\"data._json\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "          \n",
    "    # format to (input, output) pairs\n",
    "    new_data = []\n",
    "    for d in data:\n",
    "        input = (d[\"question\"], d[\"docs\"])\n",
    "        output = d[\"answer\"]\n",
    "        new_data.append((input, output))\n",
    "    return new_data[:5], None, new_data[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================================\n",
    "# Optimizer Set Up\n",
    "#================================================================\n",
    "from cognify.hub.search import default\n",
    "\n",
    "search_settings = default.create_search(n_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = load_data_minor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_cost, pareto_frontier, opt_logs = cognify.optimize(\n",
    "    script_path=\"cognify_workflow.py\",\n",
    "    control_param=search_settings,\n",
    "    train_set=train,\n",
    "    val_set=val,\n",
    "    eval_fn=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_workflow = cognify.load_workflow(config_id='Pareto_1', opt_result_path='opt_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = (\n",
    "    \"What was the 2010 population of the birthplace of Gerard Piel?\", \n",
    "    [\n",
    "        'Gerard Piel | Gerard Piel (1 March 1915 in Woodmere, N.Y. – 5 September 2004) was the publisher of the new Scientific American magazine starting in 1948. He wrote for magazines, including \"The Nation\", and published books on science for the general public. In 1990, Piel was presented with the \"In Praise of Reason\" award by the Committee for Skeptical Inquiry (CSICOP).',\n",
    "        'Woodmere, New York | Woodmere is a hamlet and census-designated place (CDP) in Nassau County, New York, United States. The population was 17,121 at the 2010 census.',\n",
    "    ],\n",
    ")\n",
    "\n",
    "new_workflow(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result = cognify.evaluate(\n",
    "    config_id='Pareto_1',\n",
    "    test_set=test,\n",
    "    opt_result_path='opt_results',\n",
    "    n_parallel=10,\n",
    "    eval_fn=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cognify.inspect(\n",
    "    'opt_results'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22501eadcd2488d944a9b516f46a160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "> Evaluation in evaluate_raw | (avg score: 0.00, avg cost@1000: 0.00 $):   0%|          | 0/10 [00:00<?, ?it/s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Testing Raw Program -----\n",
      "=========== Evaluation Results ===========\n",
      "  Quality: 0.100, Cost per 1K invocation ($): 0.16 $\n",
      "===========================================\n"
     ]
    }
   ],
   "source": [
    "eval_result = cognify.evaluate(\n",
    "    config_id='NoChange',\n",
    "    test_set=test,\n",
    "    workflow='cognify_workflow.py',\n",
    "    n_parallel=10,\n",
    "    eval_fn=metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
