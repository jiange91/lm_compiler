from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI
from langgraph.graph import END, StateGraph, START

class Unit(BaseModel):
    """return the topics/passages generated by HyDE"""
    topic: str = Field(
        description="a sub-topic generated"
    )
    passages: str = Field(
        description="passages generated for the sub-topic"
    )

class DirectHyde(BaseModel):
    """return the topics/passages generated by HyDE"""
    tps: list[Unit] = Field(
        description="a list of sub-topics and their corresponding passages"
    )
parser = PydanticOutputParser(pydantic_object=DirectHyde)

# HyDE document genration
template = """
You are an expert at writing a list of short passages given a user query. You should give sub-queries that cover comprehensive aspects of the original query and should not have too many overlaps. Each sub-query should be followed by a short passage that answers that topic.
Please answer with json format as follows:
{format_instructions}

User query: {question}
Answer:
"""
prompt_hyde = ChatPromptTemplate.from_template(template).partial(format_instructions=parser.get_format_instructions())


def direct_hyde_kernel(question):
    llm = direct_hyde_kernel.lm
    sllm = llm.with_structured_output(DirectHyde)
    hyde = prompt_hyde | sllm 
    tps = hyde.invoke({"question": question}).tps
    sub_questions, passages = [], []
    for tp in tps:
        sub_questions.append(tp.topic)
        passages.append(tp.passages)
    return {'sub_questions': sub_questions, 'passages': passages}

list_format = parser.get_format_instructions()

template = """
You are an expert at writing a list of short passages given a user query.
Please answer with json format as follows:
{format_instructions}
User query: {question}
Answer:
"""
llm = ChatOpenAI(model="gpt-4o")
def generate_passages(question):
    prompt = template.format(list_format, question)
    return {'passages': llm(prompt)}


workflow = StateGraph()
retrieve = None
filter_documents = None
knowledge_curation = None
compose_answer = None
grader = None


# Define the workload
workflow.add_node("generate_passages", generate_passages)
workflow.add_node("retrieve", retrieve)
workflow.add_node("filter_documents", filter_documents)
workflow.add_node("knowledge curation", knowledge_curation)
workflow.add_node("compose_answer", compose_answer)
workflow.add_node("grader", grader)

workflow.add_edge(START, "generate_passages")
workflow.add_edge("generate_passages", "retrieve")
workflow.add_edge("retrieve", "filter_documents")
workflow.add_edge("filter_documents", "knowledge curation")
workflow.add_edge("knowledge curation", "compose_answer")
workflow.add_conditional_edges("compose_answer", "grader", 
                               {"answer_not_grounded": "compose_answer", 
                                'answer_not_related': "generate_passages",
                                "success": END})